{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEZfFonmvr66",
        "outputId": "cda5a58a-5c71-4e1a-ac35-77e6b4bc7503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] english_wordnet..... Open English Wordnet\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "Hit Enter to continue: \n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "Hit Enter to continue: \n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] punkt_tab........... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets (JSON)\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download_shell()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQw3FO9svr68",
        "outputId": "44d087c9-94a6-420c-b734-1a588b18b3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUumn_QJvr68",
        "outputId": "3ec2ff30-26dd-4376-ed2d-4a79f3790ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGyikaPrvr68",
        "outputId": "acfdb8e8-870c-48b5-c95e-2050fb9da764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \", gb.fileids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P8E2JsI9vr69"
      },
      "outputs": [],
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8P6Ls4yvr69",
        "outputId": "3af44857-fd52-47f6-ad9b-875eeb3da57f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(macbeth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoO5N8Hovr69",
        "outputId": "81721429-9a8e-4ed4-f7d3-b92d09da0bc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "macbeth [:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vMRR4DDvr69",
        "outputId": "b60a1bbd-0fa5-4b8a-c1c8-d98d3015d941"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[',\n",
              "  'The',\n",
              "  'Tragedie',\n",
              "  'of',\n",
              "  'Macbeth',\n",
              "  'by',\n",
              "  'William',\n",
              "  'Shakespeare',\n",
              "  '1603',\n",
              "  ']'],\n",
              " ['Actus', 'Primus', '.'],\n",
              " ['Scoena', 'Prima', '.'],\n",
              " ['Thunder', 'and', 'Lightning', '.'],\n",
              " ['Enter', 'three', 'Witches', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOQKwGmNvr69",
        "outputId": "06881225-1c67-4d1b-9f46-a7de01bfb500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ],
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFr_3MSqvr6-",
        "outputId": "fc38f2c6-6b24-4229-aa6b-b63516c223e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ],
      "source": [
        "text.common_contexts(['Stage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh7fIhP1vr6-",
        "outputId": "1a5e6f8b-eb40-4a1f-ac31-2c66245d6be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ]
        }
      ],
      "source": [
        "text.similar('Stage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KObBMMrYvr6-",
        "outputId": "0f95e88d-a415-4de1-f172-a56490783c49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUpJQ_pEvr6-",
        "outputId": "a4bce355-a52c-4c8c-838f-522619f81c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl71BDNpvr6-",
        "outputId": "fab07f82-7577-4fa5-fb55-17ee5fa16951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and',\n",
              " 'before',\n",
              " \"they'll\",\n",
              " 'weren',\n",
              " 'wasn',\n",
              " 'of',\n",
              " 'your',\n",
              " \"hadn't\",\n",
              " 'does',\n",
              " 'should']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG-vuzb3vr6-",
        "outputId": "1ab0d537-b260-4a80-ce1f-55f79bf6930e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vORpAiXHvr6_",
        "outputId": "2f677c7a-2a06-4549-8417-733ac4511ccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY0jPxZfvr6_",
        "outputId": "af27e484-a4bf-4919-eb7e-21b7f628629c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ZZUPJIvr6_",
        "outputId": "2fa49981-ec61-4d88-dec8-cae8af9faa2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6hvdLsFvr6_",
        "outputId": "0dfde76a-e667-417e-b513-e4d2cae8b2ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M26Ep005vr6_",
        "outputId": "8489bb92-2b48-45b4-c149-8c4aa9cf0ab4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ99nDKPvr6_",
        "outputId": "90aa9d3d-2c6b-48be-8fc3-b36e43b3bbf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
        "tgrms.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kR26gYcpvr6_",
        "outputId": "ac833b1c-89ee-4ea3-e774-11c6b4ef13dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dUCyMNmVvr6_",
        "outputId": "299d9a79-cb50-4629-cb65-0229d01fec1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "raw[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRcLunzgvr6_",
        "outputId": "38033daa-b578-4027-8c48-d353a8ce3f90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['*',\n",
              " '*',\n",
              " '*',\n",
              " 'START',\n",
              " 'OF',\n",
              " 'THE',\n",
              " 'PROJECT',\n",
              " 'GUTENBERG',\n",
              " 'EBOOK',\n",
              " '2554',\n",
              " '*',\n",
              " '*']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q_gFbPmLvr6_",
        "outputId": "fc185e0c-6362-47cb-ccb8-ad891dcdacd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sHHhMM9Vvr7A"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM1_JqLbwvRZ",
        "outputId": "c704be3c-78d0-4170-c96e-a2b786a73958"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "for category in reviews.categories()\n",
        "for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "uya25Lk8wzzk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_review = ' '.join(documents[0][0])\n",
        "print(first_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM_WM0pbw95I",
        "outputId": "c2e675a9-cfd9-4710-bfdd-85c4b2bfc587"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "well , i ' ll admit when i first heard about this film ( which was before dante ' s peak ) i could just smell what ' twister ' had started . so now it seems that we ' re in a full fledged return to the ' 70 ' s disaster film era . with ' twister ' , ' dante ' s peak ' , ' volcano ' and soon ' flood ' and james cameron ' s extremly expensive ' titanic ' i ' ll say first off i enjoyed ' volcano ' much more than i did ' dantes peak ' . part of that is just being biased as i usually enjoy anything that tommy lee jones appears in . i ' ve been reading a lot about the movie being ' cheezy ' or ' hokey ' and really didn ' t see or think that once during the entire film . i get bored easily and this film didn ' t bore me for a minute . another thing , and boy am i bitching today ! , that bothers me is when critics and such go on about \" well thats not really possible . \" you know what ? i couldn ' t give a bloody damn if it ' s possible or not , really i couldn ' t . it ' s a freaking movie and i don ' t want reality thrown in my face . i want to be able to see the impossible , the all might \" what if \" to many reviewers have seen one to many movies ! the special effects are the second star to this feature . they are so amazing that i found it hard sometimes to believe they were indeed fx and la was was not burning to the ground . they did an incredible job and come oscar time ( if they remember ) some notice better be thrown there way .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2NtZ6fpOxJn1",
        "outputId": "67395ca0-dc74-4d60-cd92-cb383d3363a8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ],
      "metadata": {
        "id": "QBBnvcNlxaXv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(document, word_features):\n",
        " document_words = set(document)\n",
        " features = {}\n",
        " for word in word_features: features['{}'.format(word)] = (word in document_words)\n",
        " return features"
      ],
      "metadata": {
        "id": "-U8zMmyPxgpj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aWC0qPLxmhb",
        "outputId": "d23b766b-4341-4461-8c60-6cca3bf91e63"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "Ly3yowKdxwjA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZT_uTGEx2TV",
        "outputId": "ff5aa909-7faa-4811-a35b-8aee119312a7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMLWkBkOyGyu",
        "outputId": "2813eeb4-1dad-4189-b5a5-5529407e9f89"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                   loved = True              pos : neg    =     12.1 : 1.0\n",
            "              ridiculous = True              neg : pos    =     11.3 : 1.0\n",
            "                    dull = True              neg : pos    =     11.0 : 1.0\n",
            "                 titanic = True              pos : neg    =     10.1 : 1.0\n",
            "            breathtaking = True              pos : neg    =      9.4 : 1.0\n",
            "                    warm = True              pos : neg    =      8.7 : 1.0\n",
            "                designed = True              neg : pos    =      8.6 : 1.0\n",
            "            embarrassing = True              neg : pos    =      8.0 : 1.0\n",
            "                trailers = True              neg : pos    =      8.0 : 1.0\n",
            "               fashioned = True              pos : neg    =      8.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bài tập về nhà\n"
      ],
      "metadata": {
        "id": "3Re6t5DF3QQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus."
      ],
      "metadata": {
        "id": "8LyyxDKE3Ve-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "id": "sCsDe7sh3gau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb842a52-e2bf-43b1-bb4e-ddcdd46e068f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVvLoU68i-Xj",
        "outputId": "5fbdc4a4-78bf-4257-9cbc-b38c56b1c89e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các ngôn ngữ khác nhau."
      ],
      "metadata": {
        "id": "MOoQgAHGjcPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_9iWW3LjixZ",
        "outputId": "7ddddc06-dc95-48f8-a856-69c508c7d410"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "# Liệt kê danh sách các ngôn ngữ có stopwords trong NLTK\n",
        "languages = stopwords.fileids()\n",
        "print(\"Danh sách các ngôn ngữ có stopwords trong NLTK:\")\n",
        "languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuxgMICxjrtJ",
        "outputId": "6d91fc67-b9f3-4815-9490-a5e18dbd9bc3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Danh sách các ngôn ngữ có stopwords trong NLTK:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['albanian',\n",
              " 'arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'belarusian',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'tamil',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiển thị một số stopwords của từng ngôn ngữ\n",
        "for lang in languages:\n",
        "    print(f\"\\nStopwords của ngôn ngữ '{lang}':\")\n",
        "    print(stopwords.words(lang)[:10])  # Hiển thị 10 stopwords đầu tiên"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCArlBfsj9D0",
        "outputId": "f58c2d13-ccbe-40f3-9f9a-d46912663e4c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stopwords của ngôn ngữ 'albanian':\n",
            "['tyre', 'rreth', 'le', 'atyre', 'këta', 'megjithëse', 'kemi', 'per', 'ndonëse', 'dytë']\n",
            "\n",
            "Stopwords của ngôn ngữ 'arabic':\n",
            "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']\n",
            "\n",
            "Stopwords của ngôn ngữ 'azerbaijani':\n",
            "['a', 'ad', 'altı', 'altmış', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax']\n",
            "\n",
            "Stopwords của ngôn ngữ 'basque':\n",
            "['ahala', 'aitzitik', 'al', 'ala ', 'alabadere', 'alabaina', 'alabaina', 'aldiz ', 'alta', 'amaitu']\n",
            "\n",
            "Stopwords của ngôn ngữ 'belarusian':\n",
            "['на', 'не', 'што', 'па', 'да', 'за', 'як', 'для', 'гэта', 'ад']\n",
            "\n",
            "Stopwords của ngôn ngữ 'bengali':\n",
            "['অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক', 'অনেকে', 'অনেকেই', 'অন্তত', 'অন্য', 'অবধি']\n",
            "\n",
            "Stopwords của ngôn ngữ 'catalan':\n",
            "['a', 'abans', 'ací', 'ah', 'així', 'això', 'al', 'aleshores', 'algun', 'alguna']\n",
            "\n",
            "Stopwords của ngôn ngữ 'chinese':\n",
            "['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时']\n",
            "\n",
            "Stopwords của ngôn ngữ 'danish':\n",
            "['og', 'i', 'jeg', 'det', 'at', 'en', 'den', 'til', 'er', 'som']\n",
            "\n",
            "Stopwords của ngôn ngữ 'dutch':\n",
            "['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij']\n",
            "\n",
            "Stopwords của ngôn ngữ 'english':\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "\n",
            "Stopwords của ngôn ngữ 'finnish':\n",
            "['olla', 'olen', 'olet', 'on', 'olemme', 'olette', 'ovat', 'ole', 'oli', 'olisi']\n",
            "\n",
            "Stopwords của ngôn ngữ 'french':\n",
            "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n",
            "\n",
            "Stopwords của ngôn ngữ 'german':\n",
            "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']\n",
            "\n",
            "Stopwords của ngôn ngữ 'greek':\n",
            "['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ']\n",
            "\n",
            "Stopwords của ngôn ngữ 'hebrew':\n",
            "['אני', 'את', 'אתה', 'אנחנו', 'אתן', 'אתם', 'הם', 'הן', 'היא', 'הוא']\n",
            "\n",
            "Stopwords của ngôn ngữ 'hinglish':\n",
            "['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab']\n",
            "\n",
            "Stopwords của ngôn ngữ 'hungarian':\n",
            "['a', 'ahogy', 'ahol', 'aki', 'akik', 'akkor', 'alatt', 'által', 'általában', 'amely']\n",
            "\n",
            "Stopwords của ngôn ngữ 'indonesian':\n",
            "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n",
            "\n",
            "Stopwords của ngôn ngữ 'italian':\n",
            "['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n",
            "\n",
            "Stopwords của ngôn ngữ 'kazakh':\n",
            "['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ']\n",
            "\n",
            "Stopwords của ngôn ngữ 'nepali':\n",
            "['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने']\n",
            "\n",
            "Stopwords của ngôn ngữ 'norwegian':\n",
            "['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er']\n",
            "\n",
            "Stopwords của ngôn ngữ 'portuguese':\n",
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n",
            "\n",
            "Stopwords của ngôn ngữ 'romanian':\n",
            "['a', 'abia', 'acea', 'aceasta', 'această', 'aceea', 'aceeasi', 'acei', 'aceia', 'acel']\n",
            "\n",
            "Stopwords của ngôn ngữ 'russian':\n",
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']\n",
            "\n",
            "Stopwords của ngôn ngữ 'slovene':\n",
            "['ali', 'ampak', 'bodisi', 'in', 'kajti', 'marveč', 'namreč', 'ne', 'niti', 'oziroma']\n",
            "\n",
            "Stopwords của ngôn ngữ 'spanish':\n",
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n",
            "\n",
            "Stopwords của ngôn ngữ 'swedish':\n",
            "['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på']\n",
            "\n",
            "Stopwords của ngôn ngữ 'tajik':\n",
            "['аз', 'дар', 'ба', 'бо', 'барои', 'бе', 'то', 'ҷуз', 'пеши', 'назди']\n",
            "\n",
            "Stopwords của ngôn ngữ 'tamil':\n",
            "['அங்கு', 'அங்கே', 'அடுத்த', 'அதனால்', 'அதன்', 'அதற்கு', 'அதிக', 'அதில்', 'அது', 'அதே']\n",
            "\n",
            "Stopwords của ngôn ngữ 'turkish':\n",
            "['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các\n",
        "ngôn ngữ khác nhau."
      ],
      "metadata": {
        "id": "0uFRN43AkUMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm kiểm tra xem một từ có phải là stopword trong các ngôn ngữ khác nhau không\n",
        "def check_stopword(word):\n",
        "    results = {}\n",
        "    for lang in languages:\n",
        "        if word.lower() in stopwords.words(lang):\n",
        "            results[lang] = True\n",
        "        else:\n",
        "            results[lang] = False\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Mwmsg6J7kdBh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhập từ cần kiểm tra\n",
        "word_to_check = input(\"Nhập từ cần kiểm tra: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM5QtRT2kpUd",
        "outputId": "29c40c3c-0262-4a1a-b827-50ab41d72c72"
      },
      "execution_count": 44,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nhập từ cần kiểm tra: hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra và hiển thị kết quả\n",
        "results = check_stopword(word_to_check)\n",
        "print(\"\\nKết quả kiểm tra stopword:\")\n",
        "for lang, is_stopword in results.items():\n",
        "    if is_stopword:\n",
        "        print(f\"Yes: '{word_to_check}' là stopword trong ngôn ngữ: {lang}\")\n",
        "    else:\n",
        "        print(f\"No : '{word_to_check}' không phải là stopword trong ngôn ngữ: {lang}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X86oVB3zktSO",
        "outputId": "fe747352-d90d-413c-c2a2-d033ff9b220c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kết quả kiểm tra stopword:\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: albanian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: arabic\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: azerbaijani\n",
            "Yes: 'hi' là stopword trong ngôn ngữ: basque\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: belarusian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: bengali\n",
            "Yes: 'hi' là stopword trong ngôn ngữ: catalan\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: chinese\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: danish\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: dutch\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: english\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: finnish\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: french\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: german\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: greek\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: hebrew\n",
            "Yes: 'hi' là stopword trong ngôn ngữ: hinglish\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: hungarian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: indonesian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: italian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: kazakh\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: nepali\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: norwegian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: portuguese\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: romanian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: russian\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: slovene\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: spanish\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: swedish\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: tajik\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: tamil\n",
            "No : 'hi' không phải là stopword trong ngôn ngữ: turkish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho"
      ],
      "metadata": {
        "id": "wKSdjl_YlTQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def remove_stopwords(text, lang='english'):\n",
        "    stop_words = set(stopwords.words(lang))  # Lấy danh sách stopwords của ngôn ngữ\n",
        "    words = word_tokenize(text)  # Tách văn bản thành danh sách từ\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Loại bỏ stopwords\n",
        "    return ' '.join(filtered_words)  # Ghép lại thành văn bản mới"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y64o2xLrlnXE",
        "outputId": "bef83cda-d9e5-4441-a2f5-607c52a9b298"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYGW1SFJmC0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "text=raw[:200]"
      ],
      "metadata": {
        "id": "McuXvGwEmRO8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loại bỏ stopwords trong tiếng Anh\n",
        "filtered_text = remove_stopwords(text, 'english')"
      ],
      "metadata": {
        "id": "Pw2qGoB0mQRe"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiển thị kết quả\n",
        "print(\"Văn bản gốc:\", text)\n",
        "print(\"Văn bản sau khi loại bỏ stopwords:\", filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqDemim8mnRt",
        "outputId": "59de149b-56ba-49d6-8e0e-0379607fcff4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Văn bản gốc: *** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CRIME AND PUNISHMENT\n",
            "\n",
            "By Fyodor Dostoevsky\n",
            "\n",
            "\n",
            "\n",
            "Translated By Constance Garnett\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TRANSLATOR’S PREFACE\n",
            "\n",
            "A few words about Dostoevsky himself may h\n",
            "Văn bản sau khi loại bỏ stopwords: * * * START PROJECT GUTENBERG EBOOK 2554 * * * CRIME PUNISHMENT Fyodor Dostoevsky Translated Constance Garnett TRANSLATOR ’ PREFACE words Dostoevsky may h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 5:\n"
      ],
      "metadata": {
        "id": "kGK7cT7o3Nn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chọn ngôn ngữ và lấy danh sách stopwords gốc\n",
        "language = 'english'\n",
        "original_stopwords = set(stopwords.words(language))\n",
        "\n",
        "# Danh sách từ cần giữ lại (bỏ khỏi danh sách stopwords)\n",
        "words_to_keep = {'not', 'no', 'but'}\n",
        "\n",
        "# Cập nhật danh sách stopwords bằng cách loại bỏ các từ cần giữ lại\n",
        "filtered_stopwords = original_stopwords - words_to_keep\n",
        "\n",
        "# Hiển thị kết quả\n",
        "print(\"Stopwords gốc:\", sorted(original_stopwords)[:20])  # In 10 từ đầu tiên\n",
        "print(\"Stopwords sau khi lọc:\", sorted(filtered_stopwords)[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6010TmWnNbx",
        "outputId": "677d6db0-a317-484e-8d2f-fc1e968685ed"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords gốc: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
            "Stopwords sau khi lọc: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 6"
      ],
      "metadata": {
        "id": "Q5hOBxXi3TkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Tải WordNet\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_hZUv9qoSD2",
        "outputId": "4a1fc20d-8abf-4e6c-edf5-361c0d744d96"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tìm định nghĩa và ví dụ của một từ\n",
        "def get_word_info(word):\n",
        "    synsets = wordnet.synsets(word)  # Lấy danh sách các synset của từ\n",
        "    if not synsets:\n",
        "        print(f\"Không tìm thấy thông tin cho từ: {word}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Thông tin cho từ: {word}\\n\")\n",
        "    for syn in synsets[:3]:  # Chỉ lấy 3 synset đầu tiên để tránh quá dài\n",
        "        print(f\"- Định nghĩa: {syn.definition()}\")\n",
        "        if syn.examples():\n",
        "            print(f\"  Ví dụ: {syn.examples()[0]}\")\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "ktycWIBAobRA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhập từ cần tra cứu\n",
        "word = input(\"Nhập từ cần tìm định nghĩa và ví dụ: \").strip().lower()\n",
        "\n",
        "# Gọi hàm tìm kiếm\n",
        "get_word_info(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HcdZLUMohJl",
        "outputId": "248a7d10-6442-42d1-b294-7eb7b9f9b623"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhập từ cần tìm định nghĩa và ví dụ: hello\n",
            "Thông tin cho từ: hello\n",
            "\n",
            "- Định nghĩa: an expression of greeting\n",
            "  Ví dụ: every morning they exchanged polite hellos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CÂU 7."
      ],
      "metadata": {
        "id": "Xafhu-jV4h8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms_antonyms(word):\n",
        "    synonyms = set()\n",
        "    antonyms = set()\n",
        "\n",
        "    # Lấy tất cả synsets của từ\n",
        "    for syn in wordnet.synsets(word):\n",
        "        # Thêm từ đồng nghĩa từ lemmas\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())  # Thêm tên của lemma\n",
        "            # Nếu lemma có từ trái nghĩa, thêm vào antonyms\n",
        "            if lemma.antonyms():\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "    return synonyms, antonyms"
      ],
      "metadata": {
        "id": "NlL3ATOF4kMy"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhập từ cần tìm đồng nghĩa và trái nghĩa\n",
        "word = \"happy\"\n",
        "\n",
        "synonyms, antonyms = get_synonyms_antonyms(word)\n",
        "\n",
        "print(f\"Từ đồng nghĩa của '{word}': {synonyms}\")\n",
        "print(f\"Từ trái nghĩa của '{word}': {antonyms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhq6W1gg4-cM",
        "outputId": "dbb34eac-542e-4d7a-93a7-bef5705be1da"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Từ đồng nghĩa của 'happy': {'felicitous', 'happy', 'glad', 'well-chosen'}\n",
            "Từ trái nghĩa của 'happy': {'unhappy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "câu 8."
      ],
      "metadata": {
        "id": "ySTgGiOa3gI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.data import load\n",
        "import re\n",
        "\n",
        "# Tải bộ tagset\n",
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IAHCKNZpZ14",
        "outputId": "eee43a44-14e5-4abc-a741-32162f046e4b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm lấy tổng quan về tất cả các bộ tag\n",
        "def overview_tagset():\n",
        "    print(\"Tổng quan về bộ tag trong NLTK:\\n\")\n",
        "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "    for tag in list(tagdict.keys())[:10]:  # Hiển thị 10 tag đầu tiên\n",
        "        print(f\"{tag}: {tagdict[tag][0]}\")"
      ],
      "metadata": {
        "id": "wjnivtV7pc-p"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm lấy chi tiết của một tag cụ thể\n",
        "def tag_details(tag):\n",
        "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "    if tag in tagdict:\n",
        "        print(f\"\\nChi tiết về tag '{tag}':\")\n",
        "        print(f\"- Mô tả: {tagdict[tag][0]}\")\n",
        "        print(f\"- Ví dụ: {tagdict[tag][1]}\")\n",
        "    else:\n",
        "        print(f\"\\nKhông tìm thấy thông tin cho tag '{tag}'.\")"
      ],
      "metadata": {
        "id": "y4NdKhQlpkXn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tìm các tag liên quan dựa trên regex\n",
        "def search_tags(pattern):\n",
        "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "    print(f\"\\nCác tag liên quan với pattern '{pattern}':\")\n",
        "    regex = re.compile(pattern)\n",
        "    matches = [tag for tag in tagdict.keys() if regex.match(tag)]\n",
        "    if matches:\n",
        "        for tag in matches:\n",
        "            print(f\"{tag}: {tagdict[tag][0]}\")\n",
        "    else:\n",
        "        print(\"Không tìm thấy tag nào phù hợp.\")"
      ],
      "metadata": {
        "id": "Q9TkgPdtplGU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chạy chương trình\n",
        "if __name__ == \"__main__\":\n",
        "    overview_tagset()  # Hiển thị danh sách POS tags\n",
        "    tag_details('NN')  # Xem chi tiết của danh từ số ít (NN)  có thể đổi sang các (\"JJ, VB,...\")\n",
        "    search_tags(r'VB.*')  # Tìm tất cả các tag liên quan đến động từ (bắt đầu với \"VB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr3GOlM9prlZ",
        "outputId": "9c67cdb5-386d-439b-9f2d-6c4623e36b3c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng quan về bộ tag trong NLTK:\n",
            "\n",
            "PRP$: pronoun, possessive\n",
            "VBG: verb, present participle or gerund\n",
            "FW: foreign word\n",
            "VB: verb, base form\n",
            "POS: genitive marker\n",
            "'': closing quotation mark\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "VBN: verb, past participle\n",
            "JJ: adjective or numeral, ordinal\n",
            "WP: WH-pronoun\n",
            "\n",
            "Chi tiết về tag 'NN':\n",
            "- Mô tả: noun, common, singular or mass\n",
            "- Ví dụ: common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ... \n",
            "\n",
            "Các tag liên quan với pattern 'VB.*':\n",
            "VBG: verb, present participle or gerund\n",
            "VB: verb, base form\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "VBN: verb, past participle\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "VBD: verb, past tense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CÂU 9."
      ],
      "metadata": {
        "id": "8M3DprR55NnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity(noun1, noun2):\n",
        "    # Lấy synset đầu tiên của mỗi danh từ\n",
        "    syn1 = wordnet.synsets(noun1, pos=wordnet.NOUN)\n",
        "    syn2 = wordnet.synsets(noun2, pos=wordnet.NOUN)\n",
        "    if not syn1 or not syn2:\n",
        "        return None  # Nếu không tìm thấy synset cho một trong hai từ\n",
        "    # Chọn synset đầu tiên (ý nghĩa phổ biến nhất)\n",
        "    syn1 = syn1[0]\n",
        "    syn2 = syn2[0]\n",
        "    # Tính toán độ tương đồng bằng Wu-Palmer Similarity\n",
        "    similarity = syn1.wup_similarity(syn2)\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "Hzie8E1BvZ0Y"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhập hai danh từ cần so sánh\n",
        "noun1 = \"car\"\n",
        "noun2 = \"bus\"\n",
        "\n",
        "similarity_score = get_similarity(noun1, noun2)\n",
        "\n",
        "if similarity_score is not None:\n",
        "    print(f\"Độ tương đồng giữa '{noun1}' và '{noun2}' là: {similarity_score:.2f}\")\n",
        "else:\n",
        "    print(f\"Không tìm thấy thông tin để so sánh '{noun1}' và '{noun2}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb4x_ycNze27",
        "outputId": "0477fed0-cc88-44dd-83fc-1a28162c86a8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Độ tương đồng giữa 'car' và 'bus' là: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai động từ đã cho"
      ],
      "metadata": {
        "id": "PD8aysyaz06A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity(verb1, verb2):\n",
        "    # Lấy synset đầu tiên của mỗi động từ\n",
        "    syn1 = wordnet.synsets(verb1, pos=wordnet.VERB)\n",
        "    syn2 = wordnet.synsets(verb2, pos=wordnet.VERB)\n",
        "\n",
        "    if not syn1 or not syn2:\n",
        "        return None  # Nếu không tìm thấy synset cho một trong hai từ\n",
        "\n",
        "    # Chọn synset đầu tiên (ý nghĩa phổ biến nhất)\n",
        "    syn1 = syn1[0]\n",
        "    syn2 = syn2[0]\n",
        "\n",
        "    # Tính toán độ tương đồng bằng Wu-Palmer Similarity\n",
        "    similarity = syn1.wup_similarity(syn2)\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "tCnZ9Fcez5Mq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhập hai động từ cần so sánh\n",
        "verb1 = \"run\"\n",
        "verb2 = \"walk\"\n",
        "\n",
        "similarity_score = get_similarity(verb1, verb2)\n",
        "\n",
        "if similarity_score is not None:\n",
        "    print(f\"Độ tương đồng giữa '{verb1}' và '{verb2}' là: {similarity_score:.2f}\")\n",
        "else:\n",
        "    print(f\"Không tìm thấy thông tin để so sánh '{verb1}' và '{verb2}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWlQjls90rUv",
        "outputId": "99e4aab4-0420-4e24-9156-1e7d0cec2130"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Độ tương đồng giữa 'run' và 'walk' là: 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Viết chương trình Python với thư viện NLTK để tìm số lượng tên nam và nữ trong các tên\n",
        "kho ngữ liệu. In tên 10 nam và nữ đầu tiên. Lưu ý: Kho văn bản tên chứa tổng cộng khoảng\n",
        "2943 nam (male.txt) và 5001 nữ (Female.txt) tên. Kho được biên soạn bởi Kantrowitz, Ross."
      ],
      "metadata": {
        "id": "ftXTK4E71eEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Tải dữ liệu tên nếu chưa có\n",
        "nltk.download('names')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7R9afUa1fVZ",
        "outputId": "d8d33e33-ba07-4baa-ad05-bee65dbe9f95"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy danh sách tên từ kho dữ liệu\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')"
      ],
      "metadata": {
        "id": "loUJxi6e5eOt"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đếm số lượng tên nam và nữ\n",
        "num_male = len(male_names)\n",
        "num_female = len(female_names)"
      ],
      "metadata": {
        "id": "1SU_9P4f5r1w"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In kết quả\n",
        "print(f\"Số lượng tên nam: {num_male}\")\n",
        "print(f\"Số lượng tên nữ: {num_female}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZAx_HGm5v_8",
        "outputId": "b898ab2b-2327-4998-aa9f-4371bbdde41f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng tên nam: 2943\n",
            "Số lượng tên nữ: 5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In 10 tên đầu tiên của mỗi danh sách\n",
        "print(\"\\n10 tên nam đầu tiên:\", male_names[:10])\n",
        "print(\"10 tên nữ đầu tiên:\", female_names[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlpKpXvG52IV",
        "outputId": "076b21de-360d-4c47-e746-bdd8f813f63b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 tên nam đầu tiên: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
            "10 tên nữ đầu tiên: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CÂU 12."
      ],
      "metadata": {
        "id": "0Z-psnrS6OhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Tạo danh sách kết hợp (name, label)\n",
        "labeled_names = [(name, 'male') for name in male_names] + [(name, 'female') for name in female_names]"
      ],
      "metadata": {
        "id": "MeNAztcD6QKP"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xáo trộn ngẫu nhiên danh sách\n",
        "random.shuffle(labeled_names)"
      ],
      "metadata": {
        "id": "8b6An7Hu6cT7"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In 15 kết hợp ngẫu nhiên đầu tiên\n",
        "print(\"15 kết hợp ngẫu nhiên đầu tiên:\")\n",
        "for name, label in labeled_names[:15]:\n",
        "    print(f\"{name}: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaAnerNc6hgR",
        "outputId": "10017ac1-c16b-4ff7-9516-9856cd656ca5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 kết hợp ngẫu nhiên đầu tiên:\n",
            "Micheal: male\n",
            "Jerrylee: female\n",
            "Adriena: female\n",
            "Sophronia: female\n",
            "Farra: female\n",
            "Dale: female\n",
            "Irvine: male\n",
            "Candra: female\n",
            "Ardyth: female\n",
            "Cybal: female\n",
            "Jemimah: female\n",
            "Curt: male\n",
            "Jacinda: female\n",
            "Phylys: female\n",
            "Ekaterina: female\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CÂU 13."
      ],
      "metadata": {
        "id": "IW8KNjxR6xeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo danh sách chứa ký tự cuối cùng của tên và nhãn tương ứng\n",
        "final_letter_labels = [(name[-1], 'male') for name in male_names] + [(name[-1], 'female') for name in female_names]"
      ],
      "metadata": {
        "id": "6Kxbhgir60X3"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In 10 kết quả đầu tiên\n",
        "print(\"10 ký tự cuối cùng của tên và nhãn tương ứng:\")\n",
        "for letter, label in final_letter_labels[:10]:\n",
        "    print(f\"Ký tự cuối: {letter} - Nhãn: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oimw63987B2W",
        "outputId": "cbb27d35-31aa-437e-c041-64d3ba7cfdf3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 ký tự cuối cùng của tên và nhãn tương ứng:\n",
            "Ký tự cuối: r - Nhãn: male\n",
            "Ký tự cuối: n - Nhãn: male\n",
            "Ký tự cuối: y - Nhãn: male\n",
            "Ký tự cuối: e - Nhãn: male\n",
            "Ký tự cuối: t - Nhãn: male\n",
            "Ký tự cuối: t - Nhãn: male\n",
            "Ký tự cuối: y - Nhãn: male\n",
            "Ký tự cuối: l - Nhãn: male\n",
            "Ký tự cuối: l - Nhãn: male\n",
            "Ký tự cuối: m - Nhãn: male\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}